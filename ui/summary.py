"""
–ú–æ–¥—É–ª—å –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —Å—Ç–∞—Ç–µ–π –ø–æ —Ä–∞–∑–¥–µ–ª–∞–º
"""

import logging
from typing import Dict, List, Optional
from collections import defaultdict
import time

from paper_rag.embeddings import embedding_manager
from .chat import chat_manager

logger = logging.getLogger(__name__)

def summarize_paper_by_sections(arxiv_id: str, progress_bar=None, status_text=None) -> Dict:
    """
    –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è —Å—Ç–∞—Ç—å–∏ –ø–æ —Ä–∞–∑–¥–µ–ª–∞–º
    
    Args:
        arxiv_id: ID —Å—Ç–∞—Ç—å–∏ arXiv
        progress_bar: Streamlit progress bar (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        status_text: Streamlit text element –¥–ª—è —Å—Ç–∞—Ç—É—Å–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        
    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
    """
    try:
        logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—é —Å—Ç–∞—Ç—å–∏ {arxiv_id}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∏–Ω–¥–µ–∫—Å–∞
        if not embedding_manager.chunks_metadata:
            return {
                'success': False,
                'error': 'RAG –∏–Ω–¥–µ–∫—Å –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–Ω–∞—á–∞–ª–∞ –æ–±—Ä–∞–±–æ—Ç–∞–π—Ç–µ —Å—Ç–∞—Ç—å—é.'
            }
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å
        if status_text:
            status_text.text("üîç –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —á–∞–Ω–∫–æ–≤ –ø–æ —Ä–∞–∑–¥–µ–ª–∞–º...")
        if progress_bar:
            progress_bar.progress(0.1)
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —á–∞–Ω–∫–∏ –ø–æ —Ä–∞–∑–¥–µ–ª–∞–º –¥–ª—è –¥–∞–Ω–Ω–æ–π —Å—Ç–∞—Ç—å–∏
        sections_data = _group_chunks_by_sections(arxiv_id)
        
        if not sections_data:
            return {
                'success': False,
                'error': f'–ù–µ –Ω–∞–π–¥–µ–Ω—ã —á–∞–Ω–∫–∏ –¥–ª—è —Å—Ç–∞—Ç—å–∏ {arxiv_id}'
            }
        
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(sections_data)} —Ä–∞–∑–¥–µ–ª–æ–≤ –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏")
        
        # –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–µ–∫—Ü–∏—è—Ö
        for section_title, chunks in sections_data.items():
            logger.info(f"–°–µ–∫—Ü–∏—è '{section_title}': {len(chunks)} —á–∞–Ω–∫–æ–≤")
            if chunks:
                first_chunk_metadata = chunks[0].get('metadata', {})
                logger.info(f"  –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ø–µ—Ä–≤–æ–≥–æ —á–∞–Ω–∫–∞: {list(first_chunk_metadata.keys())}")
                logger.info(f"  section_title: {first_chunk_metadata.get('section_title', 'N/A')}")
                logger.info(f"  section: {first_chunk_metadata.get('section', 'N/A')}")
        
        # –°—É–º–º–∞—Ä–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π —Ä–∞–∑–¥–µ–ª
        summarized_sections = []
        total_sections = len(sections_data)
        
        for i, (section_title, chunks) in enumerate(sections_data.items()):
            if status_text:
                status_text.text(f"ü§ñ –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è —Ä–∞–∑–¥–µ–ª–∞: {section_title} ({i+1}/{total_sections})")
            
            progress = 0.1 + (i / total_sections) * 0.8
            if progress_bar:
                progress_bar.progress(progress)
            
            # –°—É–º–º–∞—Ä–∏–∑–∏—Ä—É–µ–º —Ä–∞–∑–¥–µ–ª
            section_summary = _summarize_section(section_title, chunks)
            
            if section_summary:
                summarized_sections.append(section_summary)
            
            # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞—Ç—å LLM
            time.sleep(0.5)
        
        # –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ
        if status_text:
            status_text.text(f"‚úÖ –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(summarized_sections)} —Ä–∞–∑–¥–µ–ª–æ–≤")
        if progress_bar:
            progress_bar.progress(1.0)
        
        logger.info(f"–°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(summarized_sections)} —Ä–∞–∑–¥–µ–ª–æ–≤")
        
        return {
            'success': True,
            'sections': summarized_sections,
            'total_sections': len(summarized_sections)
        }
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —Å—Ç–∞—Ç—å–∏ {arxiv_id}: {e}")
        return {
            'success': False,
            'error': str(e)
        }

def _group_chunks_by_sections(arxiv_id: str) -> Dict[str, List[Dict]]:
    """
    –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —á–∞–Ω–∫–æ–≤ –ø–æ —Ä–∞–∑–¥–µ–ª–∞–º –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Å—Ç–∞—Ç—å–∏
    
    Args:
        arxiv_id: ID —Å—Ç–∞—Ç—å–∏
        
    Returns:
        –°–ª–æ–≤–∞—Ä—å: {section_title: [chunks]}
    """
    sections = defaultdict(list)
    
    for chunk in embedding_manager.chunks_metadata:
        chunk_metadata = chunk.get('metadata', {})
        chunk_arxiv_id = chunk_metadata.get('arxiv_id')
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —á–∞–Ω–∫–∏ –Ω—É–∂–Ω–æ–π —Å—Ç–∞—Ç—å–∏
        if chunk_arxiv_id == arxiv_id:
            # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º –∫–∞–∫ —Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç 'section', —Ç–∞–∫ –∏ –Ω–æ–≤—ã–π 'section_title' –∏–∑ LaTeX
            section = chunk_metadata.get('section_title') or chunk_metadata.get('section', 'Unknown')
            chunk_index = chunk_metadata.get('chunk_index', 0)
            
            sections[section].append({
                'text': chunk.get('text', ''),
                'metadata': chunk_metadata,
                'chunk_index': chunk_index
            })
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º —á–∞–Ω–∫–∏ –≤ –∫–∞–∂–¥–æ–º —Ä–∞–∑–¥–µ–ª–µ –ø–æ –∏–Ω–¥–µ–∫—Å—É
    for section_title in sections:
        sections[section_title].sort(key=lambda x: x['chunk_index'])
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ä–∞–∑–¥–µ–ª—ã –ø–æ –ø–µ—Ä–≤–æ–º—É chunk_index –≤ —Ä–∞–∑–¥–µ–ª–µ
    sorted_sections = {}
    section_order = sorted(sections.items(), key=lambda x: min(chunk['chunk_index'] for chunk in x[1]))
    
    for section_title, chunks in section_order:
        sorted_sections[section_title] = chunks
    
    return sorted_sections

def _summarize_section(section_title: str, chunks: List[Dict]) -> Optional[Dict]:
    """
    –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ —Ä–∞–∑–¥–µ–ª–∞
    
    Args:
        section_title: –ù–∞–∑–≤–∞–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–∞
        chunks: –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ —Ä–∞–∑–¥–µ–ª–∞
        
    Returns:
        –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è —Ä–∞–∑–¥–µ–ª–∞ –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
    """
    try:
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ç–µ–∫—Å—Ç –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤ —Ä–∞–∑–¥–µ–ª–∞
        section_texts = []
        for chunk in chunks:
            text = chunk['text'].strip()
            if text:
                section_texts.append(text)
        
        if not section_texts:
            logger.warning(f"–ü—É—Å—Ç–æ–π —Ä–∞–∑–¥–µ–ª: {section_title}")
            return None
        
        original_text = " ".join(section_texts)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –¥–ª—è LLM (–º–∞–∫—Å 8000 —Å–∏–º–≤–æ–ª–æ–≤)
        max_length = 8000
        if len(original_text) > max_length:
            original_text = original_text[:max_length] + "... [–¢–ï–ö–°–¢ –û–ë–†–ï–ó–ê–ù]"
        
        # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è LLM
        article_metadata = {
            'title': 'Scientific Paper',
            'section': section_title,
            'total_chunks': len(chunks),
            'total_length': len(original_text)
        }
        
        logger.info(f"–°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è —Ä–∞–∑–¥–µ–ª–∞ '{section_title}': {len(chunks)} —á–∞–Ω–∫–æ–≤, {len(original_text)} —Å–∏–º–≤–æ–ª–æ–≤")
        
        # –ü–æ–ª—É—á–∞–µ–º LLM –º–æ–¥–µ–ª—å –∏–∑ chat_manager
        llm_model = chat_manager.llm_model
        
        if not llm_model or not llm_model.is_available:
            logger.warning(f"LLM –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —Ä–∞–∑–¥–µ–ª–∞ {section_title}")
            return {
                'title': section_title,
                'summary': f"‚ö†Ô∏è **LLM –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞.** –†–∞–∑–¥–µ–ª —Å–æ–¥–µ—Ä–∂–∏—Ç {len(chunks)} —á–∞—Å—Ç–µ–π —Å –æ–±—â–∏–º –æ–±—ä–µ–º–æ–º ~{len(original_text)} —Å–∏–º–≤–æ–ª–æ–≤.",
                'original_text': original_text,
                'chunks': chunks,
                'total_length': len(original_text)
            }
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—é —á–µ—Ä–µ–∑ LLM
        llm_response = llm_model.generate_summary(original_text, article_metadata)
        
        if llm_response.get('success'):
            summary = llm_response['content']
        else:
            logger.error(f"–û—à–∏–±–∫–∞ LLM –ø—Ä–∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ {section_title}: {llm_response.get('error', 'Unknown')}")
            summary = f"‚ùå **–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏.** –†–∞–∑–¥–µ–ª —Å–æ–¥–µ—Ä–∂–∏—Ç {len(chunks)} —á–∞—Å—Ç–µ–π —Å –æ–±—â–∏–º –æ–±—ä–µ–º–æ–º ~{len(original_text)} —Å–∏–º–≤–æ–ª–æ–≤."
        
        return {
            'title': section_title,
            'summary': summary,
            'original_text': original_text,
            'chunks': chunks,
            'total_length': len(original_text)
        }
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —Ä–∞–∑–¥–µ–ª–∞ {section_title}: {e}")
        return {
            'title': section_title,
            'summary': f"üí• **–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–∞–∑–¥–µ–ª–∞:** {str(e)}",
            'original_text': " ".join([chunk['text'] for chunk in chunks]),
            'chunks': chunks,
            'total_length': 0
        }
