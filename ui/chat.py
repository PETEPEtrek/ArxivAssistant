"""
Module for working with chat and dialogues on articles
"""

from typing import Dict, List, Optional
import streamlit as st
import logging
from paper_rag import rag_pipeline
from paper_rag.async_processor import async_processor
from llm_models import llm_factory, get_best_available_model
from .dialogue_manager import article_dialogue_manager
from paper_rag.embeddings import embedding_manager

logger = logging.getLogger(__name__)

class ChatManager:
    """
    Class for managing chat and dialogues
    """
    
    def __init__(self):
        self.chat_key = 'chat_history'
        self.llm_model = None
        self.current_model_info = None
        self._initialize_llm()
    
    def _initialize_llm(self):
        """
        Initialize LLM model
        """
        
        try:
            if 'selected_llm_model' in st.session_state:
                model_info = st.session_state.selected_llm_model
                self.current_model_info = model_info
                self.llm_model = llm_factory.create_llm(
                    model_info['type'], 
                    model_info['name']
                )
                if self.llm_model and self.llm_model.is_available:
                    logger.info(f"LLM model restored: {self.llm_model.model_name}")
                    return
            
            self.llm_model = get_best_available_model()
            if self.llm_model:
                if hasattr(self.llm_model, 'model_type'):
                    self.current_model_info = {
                        'type': self.llm_model.model_type,
                        'name': self.llm_model.model_name
                    }
                else:
                    if 'OpenAI' in type(self.llm_model).__name__:
                        self.current_model_info = {'type': 'openai', 'name': self.llm_model.model_name}
                    else:
                        self.current_model_info = {'type': 'ollama', 'name': self.llm_model.model_name}
                
                logger.info(f"LLM model initialized: {self.llm_model.model_name}")
            else:
                logger.warning("Failed to find available LLM model")
        except Exception as e:
            logger.error(f"LLM initialization error: {e}")
    
    def initialize_chat(self):
        """
        Initialize chat history if it doesn't exist
        """
        if self.chat_key not in st.session_state:
            st.session_state[self.chat_key] = []
    
    def add_message(self, role: str, content: str, arxiv_id: Optional[str] = None):
        """
        Add message to chat history
        
        Args:
            role: Sender role ('user' or 'assistant')
            content: Message content
            arxiv_id: ID —Å—Ç–∞—Ç—å–∏ –¥–ª—è –¥–∏–∞–ª–æ–≥–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        """
        self.initialize_chat()
        
        st.session_state[self.chat_key].append({
            'role': role,
            'content': content
        })
        
        if arxiv_id:
            article_dialogue_manager.add_message(arxiv_id, role, content)
            logger.info(f"–°–æ–æ–±—â–µ–Ω–∏–µ –¥–æ–±–∞–≤–ª–µ–Ω–æ –≤ –¥–∏–∞–ª–æ–≥ —Å—Ç–∞—Ç—å–∏ {arxiv_id}")

    def get_chat_history(self) -> List[Dict]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ —á–∞—Ç–∞
        
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π —á–∞—Ç–∞
        """
        self.initialize_chat()
        return st.session_state[self.chat_key]
    
    def get_dialogue_history(self, arxiv_id: str) -> List[Dict]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –¥–∏–∞–ª–æ–≥–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
        
        Args:
            arxiv_id: ID —Å—Ç–∞—Ç—å–∏
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–∏–∞–ª–æ–≥–∞
        """
        return article_dialogue_manager.get_dialogue_for_display(arxiv_id)

    def clear_chat(self, arxiv_id: Optional[str] = None):
        """
        –û—á–∏—Å—Ç–∫–∞ –∏—Å—Ç–æ—Ä–∏–∏ —á–∞—Ç–∞
        
        Args:
            arxiv_id: ID —Å—Ç–∞—Ç—å–∏ –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –¥–∏–∞–ª–æ–≥–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        """
        # –û—á–∏—â–∞–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é –∏—Å—Ç–æ—Ä–∏—é
        st.session_state[self.chat_key] = []
        
        # –û—á–∏—â–∞–µ–º –¥–∏–∞–ª–æ–≥ —Å—Ç–∞—Ç—å–∏ –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
        if arxiv_id:
            article_dialogue_manager.clear_article_dialogue(arxiv_id)
            logger.info(f"–î–∏–∞–ª–æ–≥ —Å—Ç–∞—Ç—å–∏ {arxiv_id} –æ—á–∏—â–µ–Ω")
    
    def display_chat_history(self, arxiv_id: Optional[str] = None):
        """
        –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ —á–∞—Ç–∞ –≤ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–µ
        
        Args:
            arxiv_id: ID —Å—Ç–∞—Ç—å–∏ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–∏–∞–ª–æ–≥–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        """
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–µ–∫—É—â—É—é –º–æ–¥–µ–ª—å
        if self.current_model_info:
            model_type = self.current_model_info['type']
            model_name = self.current_model_info['name']
            
            if model_type == 'openai':
                st.info(f"ü§ñ **–¢–µ–∫—É—â–∞—è –º–æ–¥–µ–ª—å:** OpenAI {model_name}")
            else:
                st.info(f"ü§ñ **–¢–µ–∫—É—â–∞—è –º–æ–¥–µ–ª—å:** Ollama {model_name}")
        
        if arxiv_id:
            dialogue_history = self.get_dialogue_history(arxiv_id)
        else:
            dialogue_history = self.get_chat_history()
        
        if dialogue_history:
            st.markdown("#### üí¨ –ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–∏–∞–ª–æ–≥–∞ –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞
            #if arxiv_id:
            #    stats = article_dialogue_manager.get_article_stats(arxiv_id)
            #    if stats['has_summary']:
            #        st.info(f"üìä **–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∏–∞–ª–æ–≥–∞:** {stats['total_messages']} —Å–æ–æ–±—â–µ–Ω–∏–π, "
            #               f"{stats['total_chars']} —Å–∏–º–≤–æ–ª–æ–≤, —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –∞–∫—Ç–∏–≤–Ω–∞")
            
            for i, message in enumerate(dialogue_history):
                if message.get('is_summary'):
                    with st.container():
                        st.markdown("üìã **–°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –¥–∏–∞–ª–æ–≥–∞:**")
                        st.markdown(message['content'])
                elif message['role'] == 'user':
                    with st.container():
                        st.markdown(f"**üôã –í—ã:** {message['content']}")
                else:
                    with st.container():
                        st.markdown(f"**ü§ñ –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç:** {message['content']}")
                
                if i < len(dialogue_history) - 1:
                    st.markdown("")
    
    def generate_response(self, user_input: str, article: Dict) -> str:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –≤–≤–æ–¥–∞
        
        Args:
            user_input: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            article: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å—Ç–∞—Ç—å–µ
            
        Returns:
            –û—Ç–≤–µ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
        """
        arxiv_id = article.get('arxiv_id')
        rag_status = self._check_rag_status(arxiv_id)
        
        if rag_status['processed']:
            return self._generate_rag_response(user_input, article, arxiv_id)
        elif rag_status['processing']:
            return self._generate_processing_response(rag_status)
        else:
            return self._generate_simple_response()
    
    def _check_rag_status(self, arxiv_id: str) -> Dict:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç–∞—Ç—å–∏ –≤ RAG —Å–∏—Å—Ç–µ–º–µ
        
        Args:
            arxiv_id: ID —Å—Ç–∞—Ç—å–∏ arXiv
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç—É—Å–æ–º
        """
        if not arxiv_id:
            return {'processed': False, 'processing': False}
        
        try:
            article_status = async_processor.get_article_status(arxiv_id)
            
            if article_status:
                status = article_status.get('status')
                if status == 'completed':
                    return {'processed': True, 'processing': False}
                elif status in ['queued', 'processing']:
                    return {'processed': False, 'processing': True, 'status': article_status}
            
            chunks_for_article = [
                chunk for chunk in embedding_manager.chunks_metadata 
                if chunk.get('metadata', {}).get('arxiv_id') == arxiv_id
            ]
            
            if len(chunks_for_article) > 0:
                return {'processed': True, 'processing': False}
            
            return {'processed': False, 'processing': False}
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ RAG —Å—Ç–∞—Ç—É—Å–∞: {e}")
            return {'processed': False, 'processing': False}
    
    def _generate_rag_response(self, user_input: str, article: Dict, arxiv_id: str) -> str:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º RAG —Å–∏—Å—Ç–µ–º—ã –∏ LLM
        
        Args:
            user_input: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            article: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å—Ç–∞—Ç—å–µ
            arxiv_id: ID —Å—Ç–∞—Ç—å–∏
            
        Returns:
            –û—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ RAG + LLM
        """
        try:
            logger.info(f"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è RAG+LLM –æ—Ç–≤–µ—Ç–∞ –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞: {user_input}")
            
            rag_result = rag_pipeline.query_article(user_input, arxiv_id)
            
            if not rag_result['success']:
                return self._generate_simple_response()
            
            chunk = rag_result.get('chunk', {})
            section_chunks = rag_result.get('section_chunks', [chunk])
            
            if len(section_chunks) > 1:
                section_texts = []
                for sc in section_chunks:
                    text = sc.get('text', '').strip()
                    if text:
                        section_texts.append(text)
                context_text = " ".join(section_texts)
                
                max_context_length = 3000
                if len(context_text) > max_context_length:
                    top_chunk_text = chunk.get('text', '')
                    remaining_length = max_context_length - len(top_chunk_text) - 50
                    if remaining_length > 0:
                        context_text = context_text[:remaining_length] + "... [MOST RELEVANT PART]: " + top_chunk_text
                    else:
                        context_text = top_chunk_text
            else:
                context_text = chunk.get('text', '')
            
            chunk_metadata = chunk.get('metadata', {})
            
            dialogue_context = article_dialogue_manager.get_dialogue_context(arxiv_id)
            if dialogue_context:
                logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–∏–∞–ª–æ–≥–∞ –¥–ª–∏–Ω–æ–π {len(dialogue_context)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            if self.llm_model and self.llm_model.is_available:
                article_metadata = {
                    'title': article.get('title', ''),
                    'authors': article.get('authors', []),
                    'section': chunk_metadata.get('section', 'Unknown Section'),
                    'arxiv_id': arxiv_id
                }
                
                llm_response = self.llm_model.generate_chat_response(
                    user_input, context_text, article_metadata, dialogue_context
                )
                
                if llm_response['success']:
                    if len(section_chunks) > 1:
                        section_name = chunk_metadata.get('section', 'Unknown Section')
                        header = f"ü§ñ **–û—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–µ–∫—Ü–∏–∏ '{section_name}':**\n"
                    else:
                        header = "ü§ñ **–û—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç–∞—Ç—å–∏:**\n"
                    
                    response_parts = [
                        header,
                        llm_response['content']
                    ]
                    
                    section = chunk_metadata.get('section', 'Unknown Section')
                    source_info = f"\nüìç *–ò—Å—Ç–æ—á–Ω–∏–∫: {section}*"
                    response_parts.append(source_info)
                    
                    #top_chunk_info = self._format_top_chunk_debug(chunk)
                    #if top_chunk_info:
                    #    response_parts.append(top_chunk_info)
                    
                    return "\n".join(response_parts)
            
            if len(section_chunks) > 1:
                section_name = chunk_metadata.get('section', 'Unknown Section')
                header = f"üìÑ **–ù–∞–π–¥–µ–Ω–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ —Å–µ–∫—Ü–∏–∏ '{section_name}':**\n"
            else:
                header = "üìÑ **–ù–∞–π–¥–µ–Ω–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:**\n"
            
            response_parts = [
                header,
                context_text[:500] + "..." if len(context_text) > 500 else context_text
            ]
            
            section = chunk_metadata.get('section', 'Unknown Section')
            response_parts.append(f"\nüìç *–ò—Å—Ç–æ—á–Ω–∏–∫: {section}*")
            
            #top_chunk_info = self._format_top_chunk_debug(chunk)
            #if top_chunk_info:
            #    response_parts.append(top_chunk_info)
            
            return "\n".join(response_parts)
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ RAG –æ—Ç–≤–µ—Ç–∞: {e}")
            return self._generate_simple_response()
    
    def _generate_processing_response(self, rag_status: Dict) -> str:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –≤–æ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç–∞—Ç—å–∏
        
        Args:
            rag_status: –°—Ç–∞—Ç—É—Å –æ–±—Ä–∞–±–æ—Ç–∫–∏
            
        Returns:
            –°–æ–æ–±—â–µ–Ω–∏–µ –æ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        """
        status_info = rag_status.get('status', {})
        stage = status_info.get('stage', 'processing')
        progress = status_info.get('progress', 0)
        
        stage_messages = {
            'queued': '–≤ –æ—á–µ—Ä–µ–¥–∏ –Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫—É',
            'downloading': '—Å–∫–∞—á–∏–≤–∞–Ω–∏–µ PDF',
            'extracting_text': '–∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞',
            'chunking': '—Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã',
            'embedding': '—Å–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞',
            'processing': '–æ–±—Ä–∞–±–æ—Ç–∫–∞'
        }
        
        stage_text = stage_messages.get(stage, '–æ–±—Ä–∞–±–æ—Ç–∫–∞')
        
        return f"""üîÑ **–°—Ç–∞—Ç—å—è –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –¥–ª—è —É–º–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞**

üìä **–°—Ç–∞—Ç—É—Å:** {stage_text} ({progress}%)

‚è≥ –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–¥–æ–∂–¥–∏—Ç–µ... –ü–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —è —Å–º–æ–≥—É –¥–∞–≤–∞—Ç—å –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã–µ –∏ –¥–µ—Ç–∞–ª—å–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–ª–Ω–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è —Å—Ç–∞—Ç—å–∏.

üí° –ü–æ–∫–∞ –º–æ–∂–µ—Ç–µ –∑–∞–¥–∞–≤–∞—Ç—å –æ–±—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã –æ —Å—Ç–∞—Ç—å–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏."""
    
    def _generate_simple_response(self) -> str:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ—Å—Ç–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –±–µ–∑ RAG
        
        Args:
            user_input: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            article: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å—Ç–∞—Ç—å–µ
            
        Returns:
            –ü—Ä–æ—Å—Ç–æ–π –æ—Ç–≤–µ—Ç
        """
        return "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å —Å–¥–µ–ª–∞—Ç—å RAG –æ–±—Ä–∞–±–æ—Ç–∫—É —Å—Ç–∞—Ç—å–∏, –ø–æ–ø—Ä–æ–π—Ç–∏ –ø–µ—Ä–µ–∑–∞–π—Ç–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å–æ —Å—Ç–∞—Ç—å–µ–π –∏ –ø–æ–¥–æ–∂–¥–∞—Ç—å –ø–µ—Ä–µ–¥ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã"
    
    def switch_model(self, model_info: Dict) -> bool:
        """
        –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ –¥—Ä—É–≥—É—é LLM –º–æ–¥–µ–ª—å
        
        Args:
            model_info: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏ {'type': 'openai/ollama', 'name': 'model_name'}
            
        Returns:
            True –µ—Å–ª–∏ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ
        """
        
        try:
            new_model = llm_factory.create_llm(
                model_info['type'], 
                model_info['name']
            )
            
            if new_model and new_model.is_available:
                self.llm_model = new_model
                self.current_model_info = model_info
                
                st.session_state.selected_llm_model = model_info
                
                logger.info(f"–ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ –º–æ–¥–µ–ª—å: {model_info['type']}/{model_info['name']}")
                return True
            else:
                logger.error(f"–ú–æ–¥–µ–ª—å {model_info['type']}/{model_info['name']} –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
                return False
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}")
            return False
    
    def _format_top_chunk_debug(self, chunk: Dict) -> str:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–ª–∞–¥–æ—á–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç–æ–ø —á–∞–Ω–∫–µ
        
        Args:
            chunk: –°–∞–º—ã–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π —á–∞–Ω–∫
            relevance: –ê–Ω–∞–ª–∏–∑ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            
        Returns:
            –û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        """
        try:
            chunk_metadata = chunk.get('metadata', {})
            chunk_text = chunk.get('text', '')
            score = chunk.get('score', 0)
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –æ—Ç–ª–∞–¥–æ—á–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤—Å–µ–≥–¥–∞ (–¥–ª—è –æ—Ç–ª–∞–¥–∫–∏)
            # if relevance.get('level') not in ['–Ω–∏–∑–∫–∞—è', '–æ—á–µ–Ω—å –Ω–∏–∑–∫–∞—è']:
            #     return ""
            
            debug_parts = [
                f"\nüîç **–û–¢–õ–ê–î–ö–ê - –°–∞–º—ã–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π —á–∞–Ω–∫ (Score: {score:.3f}):**",
                f"üìÇ –°–µ–∫—Ü–∏—è: {chunk_metadata.get('section', 'Unknown')}",
                f"üìù –ß–∞–Ω–∫ #{chunk_metadata.get('chunk_index', 'N/A')}",
                f"üîç –¢–∏–ø –ø–æ–∏—Å–∫–∞: {chunk.get('search_type', 'standard')}",
            ]
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≥–∏–±—Ä–∏–¥–Ω–æ–º –ø–æ–∏—Å–∫–µ –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞
            if chunk.get('search_type') == 'hybrid':
                bm25_score = chunk.get('bm25_score', 0)
                semantic_score = chunk.get('semantic_score', 0)
                debug_parts.append(f"‚ö° BM25 score: {bm25_score:.3f}, Semantic score: {semantic_score:.3f}")
            elif chunk.get('search_type') == 'bm25':
                debug_parts.append(f"üìù BM25 –ø–æ–∏—Å–∫ (–ª–µ–∫—Å–∏—á–µ—Å–∫–∏–π)")
            else:
                debug_parts.append(f"üß† –≠–º–±–µ–¥–¥–∏–Ω–≥ –ø–æ–∏—Å–∫ (—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π)")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º BM25 –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –µ—Å–ª–∏ –µ—Å—Ç—å
            bm25_candidates = chunk.get('debug_bm25_candidates', [])
            if bm25_candidates:
                debug_parts.append(f"\nüìã **BM25 –∫–∞–Ω–¥–∏–¥–∞—Ç—ã (—Ç–æ–ø-{len(bm25_candidates)}):**")
                for i, candidate in enumerate(bm25_candidates, 1):
                    cand_meta = candidate.get('metadata', {})
                    cand_section = cand_meta.get('section', 'Unknown')
                    cand_chunk = cand_meta.get('chunk_index', 'N/A')
                    cand_score = candidate.get('score', 0)
                    cand_text = candidate.get('text', '')[:100].replace('\n', ' ')
                    debug_parts.append(f"   {i}. BM25: {cand_score:.3f} | –ß–∞–Ω–∫ #{cand_chunk} | {cand_section}")
                    debug_parts.append(f"      –¢–µ–∫—Å—Ç: {cand_text}...")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ —á–∞–Ω–∫–∞
            debug_parts.append(f"\nüí¨ **–ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ —á–∞–Ω–∫–∞:**")
            debug_parts.append(f"{chunk_text}")
            
            return "\n".join(debug_parts)
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –æ—Ç–ª–∞–¥–æ—á–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏: {e}")
            return ""

chat_manager = ChatManager()
